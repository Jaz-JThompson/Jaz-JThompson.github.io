---
title: 'A Long Short-Term Memory Network for Plasma Diagnosis from Langmuir Probe Data'
date: 2022-11-03
permalink: /posts/2022/11/LSTM-NN-Langmuir/
layout: single
tags:
  - Fusion Plamsa
  - Langmuir Probes
  - Machine Learning
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
toc_stickey: true
---

# Introduction

Welcome to my latest blog post where we will be discussing the recent paper "A Long Short-Term Memory Network for Plasma Diagnosis from Langmuir Probe Data", found [Here](https://www.mdpi.com/1424-8220/22/11/4281 "A Long Short-Term Memory Network for Plasma Diagnosis from Langmuir Probe Data"). This paper, published in the Journal of Nuclear Materials and Energy, presents a new approach to diagnosing plasma using a Long Short-Term Memory (LSTM) network. As fusion physicists, I am always excited to see new techniques and ideas being applied to the field of plasma physics. In this post, I will provide a brief overview of the paper, explain the LSTM network, and give some thoughts on the potential impact of this research on the fusion field. So, let's dive in!

## Fusion plasmas… why are they important?

The development of fusion plasma devices has rapidly increased in the last 20 years, bringing us ever closer to green, sustainable energy. Plasmas as the fourth state of matter where ions and electrons are unbound and move freely from one another at high temperatures. These high temperatures and pressures maintained by various heating techniques and supercooled magnets cause nuclei to fuse, generating massive amounts of energy. However, maintaining a fusion plasma is no easy task. 
Safe ignition, maintenance and dispersion of plasma discharges are heavily reliant on the capabilities of the diagnostic sensors. As the majority of small to medum sized reactors only produce plasmas for 0.001-1 second, diagnostics must be fast, responsicve and accurate to provide live feedback control or accurate post-plasma diagnostics. Plasma parameters such as electron temperature, density and potential are calculated from data collected by these diagnostic sensors. These parameters can then be used to determine the form of the plasma, weather the plasma is stable or tending to instability e.t.c..  

Conventional diagnostic techniques for extracting the plasma parameters, such as the Langmuir probe, involve active and indirect measure of the plasma. In this case, the plasma is changed in a small area by the diagnostic to probe to obtain the characteristics from it. However, these types of measurement usually require a certain amount of offline-computation, hence cannot be used as live diagnostic tools. Additionally, external factors can greatly affect the accuracy of the plasma measurements.
In this paper aims to create a more universal method of plasma diagnosis by speeding up parameter calculation and celiminating the influence of external factors using long short-term memory (LSTM) neural networks (NN). 

## The Langmuir probe and how it works
Langmuir probes, proposed in 1926, are a commonly used diagnostic tool in plasma physics research. They essentially work by inserting a small probe into the plasma and measuring the electrical characteristics of the plasma at that point. The probe has a small wire or needle that is oscillated between being positively and negatively charged, which is inserted into the plasma. The probe measures the current, I flowing to the wire by collecting electrons from the plasma and therefore the voltage, V across it. From these measurements, we can deduce important properties of the plasma from the I-V characterisitcs, such as the electron temperature, $T_e$ and density, $n_e$. One of the great things about Langmuir probes is that they can be used to make measurements in a wide range of plasmas, from fusion plasmas to the plasma in stars. They are a simple yet powerful tool that can provide a lot of information about the plasma being studied [[1]](https://aapt.scitation.org/doi/10.1119/1.2772282 "Understanding Langmuir probe current-voltage characteristics").

<div style="text-align:center">
    <a href="https://davidpace.com/example-of-langmuir-probe-analysis/">
        <img src="/images/Langmuir_Setup.png" alt="Langmuir_Setup" style="width:70%;height:50%">
    </a>
    <br>
    <i>Figure 1: Circuit diagram of the Langmuir probe setup</i>
</div>

The aim of the paper is to determine the characterisitcs electron temperature, $T_e$ and density, $n_e$. These are found using therotical models based of a perfect plasma and fitting them to the data optained from the probe. The perfect data curve from a Langmuir probe would look a little like an 'S' like that in **fig. 2**. For those interested in how Langmuir's method finds the characterisitcs, we go into a little more mathematical detail here, otherwise feel free to skip to the next section [Problems with measurements](#Problems-with-measurements).

I-V characteristic is obtained by sweeping a bias voltage (VB) from negative to positive potentials. From these curves the plasma characteristics are calculated from specific features of the curve:

<div style="text-align:center">
    <a href="https://aapt.scitation.org/doi/10.1119/1.2772282">
        <img src="/images/Langmuir_chars.png" alt="Langmuir_chars" style="width:100%;height:100%">
    </a>
    <br>
    <i>Figure 2: Langmuir probe I-V characteristic</i>
</div>

The total current in **fig. 2** is represented by the black curve and the ionic and electron current are represented by the blue and red curves respectively. Different features of the black I-V characterisci are used to find $n_e$ and $T_e$:
  * Floating potential, $V_f$: The point where the total voltage of the probe changes sign
  * Plasma potential, $V_p$: The point where the curve bends to the right (i.e. Where all the electrons are relpelled)
  * Electron temperature, $T_e$: The gradient of the middle region **labled in blue on fig. 2** is $=e/{(k_bT_e)}$

Knowing all these values allows us to apply the following relations to determine $n_e$ using,

$$ I_{sat} = A_se^{-\frac{1}{2}}qn_e\sqrt{\frac{K_BT_e}{M}} $$

$$ n_e = \frac{I_{sat} }{qA_se^{-\frac{1}{2}}}\sqrt{\frac{M}{K_BT_e}} $$

Where the new terms here are ion mass, $M$, Aarea of the probe, $A_s$ and the physical constants, $K_B$ = Boltzmann's Constant and $e$ = electric charge.

These Characteristics are obtained post-processing and curve fitting of the data collected during experimentation. Thes relations are purely based on theory, and change depending on the probe geometry, conductivity, plasma type, e.t.c... 

<a id="Problems-with-measurements"></a>
## Problems with measurements
 Theoretical models are selten perfect and the case of Langmuir probes is no different. The I-V characteristic drastically deviates from the standard from if the surface layer of the probe is contaminated or if the probe is placed in a low-density plasma. The characteritic bends in the curve are no longer as obvious, and as the voltage sweeps from negative to positve, the curves no longer opverlap. This can make the gradient value very hard to find. A bad curve example can be seen in **Fig. 3**. 

<div style="text-align:center">
    <a href="https://www.mdpi.com/1424-8220/22/11/4281">
        <img src="/images/contaminated.png" alt="Langmuir_contam" style="width:60%;height:60%">
    </a>
    <br>
    <i>Figure 3: A contaminated probe's I-V characteristic</i>
</div>

Faced with these issues, traditional fitting methods can be even more of a challenge. Different levels of contamination and low data aquisition of low-density plasmas leads to a lot of noise in the data. This requires a lot of computational resources and can be very slow, which isn't ideal when you're trying to analyze data in real-time. So, the goal of this research was to develop a new method for diagnosing plasmas using Langmuir probe data that is faster, more accurate and doesn't suffer from the problem of probe contamination and low-density plasmas. And, it looks like they succeeded!


<div style="text-align:center">
            <img src="/images/NN_structure_graphic.png" alt="NN_graphic" style="width:100%;height:80%">
    <br>
    <i>Figure 4: The Researchers goal for the input and putput of their network</i>
</div>

# Machine Learning 
Predicting $n_e$ and $T_e$ is a simple regression problem and can be resolved with a simple multi-layer perceptron (MLP) network. These MLP networks are relatively easy to use and lightweight, but not suited to characterise messy data that could be affected by multiple factors of varying degrees (such as the level of contamination). Hence, instead of using one-way propagation system in the MLP, they use a neural network structure called long short-term memory network (LSTM). 

## LSTM , what is it and how does it work?
Now hopefully, when I mention Langmuir probes again, you have a general idea of what I’m talking about. This is because we don’t start thinking from scratch. All because we have moved on to the next section, doesn’t mean we forget everything from before. As you read the blog, you understand the context of each section based on the previous sections. You don’t throw away all your knowledge and start thinking from scratch again. There is a certain continuity to our thoughts. 

However,  traditional neural networks can’t do this. Regular neural networks have a hard time understanding sequences of data. They're good at recognizing patterns in a single data point, like identifying a prcure of a cat... as a picture of a cat. But when it comes to understanding something like a sentence or a time series, regular neural networks can struggle. Sequential networks struggle relating previous words in a sentence with the next ones, essentially, forgetting everything every time new information given. Recurrent neural networks (RNN) attempt to solve this by having loops within the network, allowing them to persist. RNNs can be thought of as multiple copies of the same traditional sequential network but every time it runs, information is carried over to the successor. 

Because of RNN’s ability to remember past information, they have been incredibly successful when applied to tasks such as translation, text generation, speech recognition… and many more. 

However, RNNs have some fatal flaws and may not be completely suitable for prediction based on information given quite a while ago. This was explored in detail by  [Hochreiter](https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf "Untersuchungen zu dynamischen neuronalen Netzen"), who proved   mathematically to why this is difficult. 

That's where LSTMs come in. LSTMS are a special kind of RNN used for any series where a sequential prediction is required, and long-term dependency is attached to it. Like all neural networks, there are repeating cell with hidden layers (e.g. activation function)  side. The LSTM cell looks like this: 

<div style="text-align:center">
            <img src="/images/cell(2).png" alt="LSTM_cell" style="width:80%;height:80%">
    <br>
</div>
<div style="text-align:center">
            <img src="/images/operations.png" alt="LSTM_operators" style="width:80%;height:80%">
    <br>
    <i>Figure 5: A general strucuture for an LSTM cell</i>
</div>

The diagram above illustrates how information flows through an LSTM network. Each line represents a vector that connects the output of one node, to the input of others. The pink circles represent simple operations like adding vectors together, while the yellow boxes represent layers of the neural network. When lines come together, it means the vectors are being combined, and when a line splits, it means the information is being copied and sent to different parts of the network.

To understand exactly what an LSTM cell is doing with the data, lets break it down into 4 parts.

<div style="text-align:center">
            <img src="/images/LSTM_breakdown_2x2.png" alt="LSTM_operators" style="width:100%;height:100%">
    <br>
    <i>Figure 6: A breakdown of an LSTM cell</i>
</div>

The memory part of the LSTM cell is the cell state vector. Information is added or removed to the cell state via the interations with the gates. Gates consisit of two parts: a sigmoid neural network layer and a pointwise multiplication operation. The sigmoid layer is used to decide how much information should be kept or discarded (by assigning a value beteen 0-1), and the pointwise multiplication operation add this information to the cell state.

The first gate controlls how much information we want to remove from the cell state, hence aptly named the 'forget gate'. It takes the output value $h_{t-1}$ from the previous cell and the new data point $x_t$ and decides how much information from the previous cell is relevent. Combinign that with the trainable weights, $W$ and biases $b$ of that layer results in $f_t$ being passed onto the cell state vector.
<img src="/images/cell_state_vec.png" alt="alt text" width="50%" height="50%"> $$f_t = (W_f \cdot [h_{t-1},x_t] + b_f) $$


Now its time do decide which information from the new data is doing to be passed on to the cell state. This is determined by the 'input gate'. The sigmoid layer chooses what to keep and which need updating and the tanh layer creates the new values for the update.












